{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dbb3bba2",
   "metadata": {},
   "source": [
    "##### Σχόλιο\n",
    "\n",
    "Χρησιμοποιήθηκε η έκδοση 4.3.1 του gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac2e00e",
   "metadata": {},
   "source": [
    "### Βήμα 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "46454a98",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['emma', 'by', 'jane', 'austen']\n"
     ]
    }
   ],
   "source": [
    "import warnings; warnings.simplefilter(\"ignore\")\n",
    "\n",
    "# a) Read corpus into tokens\n",
    "location = \"corpus/gutenberg.txt\"\n",
    "file = open(location,\"r\")\n",
    "lines = file.readlines()\n",
    "sent = [line.split() for line in lines]  \n",
    "print(sent[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ba94700a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# b) Use gensim for training\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "#trained on gutenberg.txt\n",
    "model1 = Word2Vec(sentences=sent,vector_size=100,window=5, workers = 8, epochs = 1000)\n",
    "model1.save(\"word2vec.model1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a1c5b2b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define cosine similarity (finally, we used most_similar function that uses cosine similarity)\n",
    "def cosine_distance (model, word,target_list , num) :\n",
    "    cosine_dict ={}\n",
    "    word_list = []\n",
    "    a = model[word]\n",
    "    for item in target_list :\n",
    "        if item != word :\n",
    "            b = model [item]\n",
    "            cos_sim = dot(a, b)/(norm(a)*norm(b))\n",
    "            cosine_dict[item] = cos_sim\n",
    "    dist_sort=sorted(cosine_dict.items(), key=lambda dist: dist[1],reverse = True) ## in Descedning order \n",
    "    for item in dist_sort:\n",
    "        word_list.append((item[0], item[1]))\n",
    "    return word_list[0:num]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b3e59bf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bible Similar:\n",
      "('seriousness', 0.3718793988227844)\n",
      "('dreamer', 0.36886540055274963)\n",
      "('cob', 0.3591678738594055)\n",
      "('propitious', 0.34457987546920776)\n",
      "('syrian', 0.3390393853187561)\n",
      "('counts', 0.3369024693965912)\n",
      "('poetry', 0.3312138319015503)\n",
      "('verie', 0.33115240931510925)\n",
      "('pulpit', 0.3305303156375885)\n",
      "('beads', 0.31800422072410583)\n",
      "\n",
      "Book Similar:\n",
      "('letter', 0.4842962622642517)\n",
      "('written', 0.4792977273464203)\n",
      "('temple', 0.46236783266067505)\n",
      "('note', 0.447600781917572)\n",
      "('pen', 0.4464194178581238)\n",
      "('word', 0.42215245962142944)\n",
      "('chronicles', 0.4212881922721863)\n",
      "('chapter', 0.42086586356163025)\n",
      "('mouth', 0.412833571434021)\n",
      "('seal', 0.4087667167186737)\n",
      "\n",
      "Bank Similar:\n",
      "('top', 0.5457448959350586)\n",
      "('table', 0.5084173083305359)\n",
      "('floor', 0.48206016421318054)\n",
      "('river', 0.468691885471344)\n",
      "('side', 0.4670913517475128)\n",
      "('hill', 0.46269726753234863)\n",
      "('ridge', 0.4617703855037689)\n",
      "('ground', 0.453009694814682)\n",
      "('wall', 0.4523957669734955)\n",
      "('trudged', 0.44693323969841003)\n",
      "\n",
      "Water Similar:\n",
      "('waters', 0.613674521446228)\n",
      "('wine', 0.53905189037323)\n",
      "('fire', 0.48961469531059265)\n",
      "('wood', 0.48945316672325134)\n",
      "('ground', 0.48344963788986206)\n",
      "('oil', 0.48240533471107483)\n",
      "('blood', 0.46017879247665405)\n",
      "('smoke', 0.4569704830646515)\n",
      "('hole', 0.44915759563446045)\n",
      "('fowls', 0.44907060265541077)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# c) Top 5 similar words to bank,book,bible,water\n",
    "\n",
    "bible_sim = model1.wv.most_similar(\"bible\")[:10]\n",
    "book_sim = model1.wv.most_similar(\"book\")[:10]\n",
    "bank_sim = model1.wv.most_similar(\"bank\")[:10]\n",
    "water_sim = model1.wv.most_similar(\"water\")[:10]\n",
    "\n",
    "print(\"Bible Similar:\")\n",
    "for i in bible_sim:\n",
    "    print(i)\n",
    "print()\n",
    "print(\"Book Similar:\")\n",
    "for i in book_sim:\n",
    "    print(i)\n",
    "print()\n",
    "\n",
    "print(\"Bank Similar:\")\n",
    "for i in bank_sim:\n",
    "    print(i)\n",
    "print()\n",
    "\n",
    "print(\"Water Similar:\")\n",
    "for i in water_sim:\n",
    "    print(i)\n",
    "print()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "576f185b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test for less epochs and wider window \n",
    "model1_v1 = Word2Vec(sentences=sent,vector_size=100,window=5, workers = 8, epochs = 100)\n",
    "model1_v2 = Word2Vec(sentences=sent,vector_size=100,window=5, workers = 8, epochs = 10)\n",
    "model1_v3 = Word2Vec(sentences=sent,vector_size=100,window=10, workers = 8, epochs = 10)\n",
    "model1_v4 = Word2Vec(sentences=sent,vector_size=100,window=15, workers = 8, epochs = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "6e68e67e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bible Similar:\n",
      "('dreamer', 0.4342961609363556)\n",
      "('propitious', 0.40272602438926697)\n",
      "('answerer', 0.4013298451900482)\n",
      "('pliny', 0.3979036211967468)\n",
      "('title', 0.3977380692958832)\n",
      "('poetry', 0.3628053367137909)\n",
      "('pub', 0.3592997193336487)\n",
      "('falsely', 0.3583223223686218)\n",
      "('jonas', 0.3550158143043518)\n",
      "('seer', 0.3513900935649872)\n",
      "\n",
      "Book Similar:\n",
      "('chronicles', 0.559377133846283)\n",
      "('written', 0.5343720316886902)\n",
      "('letter', 0.49462002515792847)\n",
      "('volume', 0.4642413556575775)\n",
      "('covenant', 0.46199753880500793)\n",
      "('chapter', 0.4580816924571991)\n",
      "('epistle', 0.45133349299430847)\n",
      "('mouth', 0.4467475414276123)\n",
      "('law', 0.44571366906166077)\n",
      "('headlines', 0.43450728058815)\n",
      "\n",
      "Bank Similar:\n",
      "('top', 0.5511630177497864)\n",
      "('floor', 0.525230884552002)\n",
      "('side', 0.5161129832267761)\n",
      "('wall', 0.5136275291442871)\n",
      "('ground', 0.5116709470748901)\n",
      "('table', 0.5025489330291748)\n",
      "('edge', 0.49797216057777405)\n",
      "('river', 0.4974571764469147)\n",
      "('hill', 0.4959597587585449)\n",
      "('tower', 0.4829375743865967)\n",
      "\n",
      "Water Similar:\n",
      "('waters', 0.6162407994270325)\n",
      "('river', 0.5352722406387329)\n",
      "('streams', 0.5252775549888611)\n",
      "('wine', 0.5239294171333313)\n",
      "('camels', 0.5048832893371582)\n",
      "('buckets', 0.4904658794403076)\n",
      "('fowls', 0.4903753399848938)\n",
      "('ground', 0.4889426827430725)\n",
      "('sea', 0.4685840904712677)\n",
      "('blood', 0.4673968255519867)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Results for window=5 and 100 epochs\n",
    "\n",
    "bible_sim = model1_v1.wv.most_similar(\"bible\")[:10]\n",
    "book_sim = model1_v1.wv.most_similar(\"book\")[:10]\n",
    "bank_sim = model1_v1.wv.most_similar(\"bank\")[:10]\n",
    "water_sim = model1_v1.wv.most_similar(\"water\")[:10]\n",
    "\n",
    "print(\"Bible Similar:\")\n",
    "for i in bible_sim:\n",
    "    print(i)\n",
    "print()\n",
    "print(\"Book Similar:\")\n",
    "for i in book_sim:\n",
    "    print(i)\n",
    "print()\n",
    "\n",
    "print(\"Bank Similar:\")\n",
    "for i in bank_sim:\n",
    "    print(i)\n",
    "print()\n",
    "\n",
    "print(\"Water Similar:\")\n",
    "for i in water_sim:\n",
    "    print(i)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ac71737b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bible Similar:\n",
      "('eidolons', 0.73551344871521)\n",
      "('gittite', 0.7245368957519531)\n",
      "('thanes', 0.722448468208313)\n",
      "('literatures', 0.7192274332046509)\n",
      "('dwells', 0.7189253568649292)\n",
      "('hashum', 0.7139861583709717)\n",
      "('robin', 0.7126010060310364)\n",
      "('remoue', 0.7103672623634338)\n",
      "('massy', 0.708891749382019)\n",
      "('natives', 0.7080622911453247)\n",
      "\n",
      "Book Similar:\n",
      "('chapter', 0.6683662533760071)\n",
      "('chronicles', 0.6407068371772766)\n",
      "('letter', 0.6085094213485718)\n",
      "('volume', 0.6024599671363831)\n",
      "('note', 0.5997639894485474)\n",
      "('written', 0.5634008049964905)\n",
      "('epistle', 0.549001932144165)\n",
      "('prophet', 0.5489466786384583)\n",
      "('folio', 0.5303294062614441)\n",
      "('temple', 0.5270305871963501)\n",
      "\n",
      "Bank Similar:\n",
      "('lawn', 0.8556665182113647)\n",
      "('floor', 0.8353729248046875)\n",
      "('wall', 0.8078148365020752)\n",
      "('top', 0.805381178855896)\n",
      "('beach', 0.7994846701622009)\n",
      "('hill', 0.79826819896698)\n",
      "('road', 0.7869361639022827)\n",
      "('threshold', 0.7685083746910095)\n",
      "('shore', 0.7645455598831177)\n",
      "('east', 0.7627404928207397)\n",
      "\n",
      "Water Similar:\n",
      "('wood', 0.6936861872673035)\n",
      "('smoke', 0.6536219716072083)\n",
      "('waters', 0.6452018618583679)\n",
      "('ashes', 0.6295132637023926)\n",
      "('streams', 0.6179492473602295)\n",
      "('steel', 0.6118414998054504)\n",
      "('corn', 0.6080697774887085)\n",
      "('rivers', 0.5992569923400879)\n",
      "('wine', 0.5987640619277954)\n",
      "('fire', 0.5986717939376831)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Results for window=5 and 10 epochs.\n",
    "\n",
    "bible_sim = model1_v2.wv.most_similar(\"bible\")[:10]\n",
    "book_sim = model1_v2.wv.most_similar(\"book\")[:10]\n",
    "bank_sim = model1_v2.wv.most_similar(\"bank\")[:10]\n",
    "water_sim = model1_v2.wv.most_similar(\"water\")[:10]\n",
    "\n",
    "print(\"Bible Similar:\")\n",
    "for i in bible_sim:\n",
    "    print(i)\n",
    "print()\n",
    "print(\"Book Similar:\")\n",
    "for i in book_sim:\n",
    "    print(i)\n",
    "print()\n",
    "\n",
    "print(\"Bank Similar:\")\n",
    "for i in bank_sim:\n",
    "    print(i)\n",
    "print()\n",
    "\n",
    "print(\"Water Similar:\")\n",
    "for i in water_sim:\n",
    "    print(i)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "09a78f81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bible Similar:\n",
      "('republic', 0.7240028381347656)\n",
      "('spain', 0.7129253149032593)\n",
      "('climes', 0.6993950605392456)\n",
      "('persecutions', 0.6960647106170654)\n",
      "('dwells', 0.688989520072937)\n",
      "('displays', 0.6880658268928528)\n",
      "('misapprehension', 0.6879128813743591)\n",
      "('hauing', 0.6866286993026733)\n",
      "('museum', 0.6856420040130615)\n",
      "('exhorting', 0.6823752522468567)\n",
      "\n",
      "Book Similar:\n",
      "('chapter', 0.6891915202140808)\n",
      "('chronicles', 0.6169758439064026)\n",
      "('written', 0.6071396470069885)\n",
      "('prophecy', 0.5652978420257568)\n",
      "('prophet', 0.565168023109436)\n",
      "('letter', 0.5636975765228271)\n",
      "('note', 0.5513817667961121)\n",
      "('vision', 0.5460059642791748)\n",
      "('epistle', 0.5458642840385437)\n",
      "('folio', 0.5403078198432922)\n",
      "\n",
      "Bank Similar:\n",
      "('floor', 0.8299848437309265)\n",
      "('top', 0.8209477066993713)\n",
      "('bay', 0.803145170211792)\n",
      "('road', 0.8017860651016235)\n",
      "('beach', 0.8013285994529724)\n",
      "('hill', 0.7976561188697815)\n",
      "('lawn', 0.7947807312011719)\n",
      "('hedge', 0.7822613716125488)\n",
      "('path', 0.7777345776557922)\n",
      "('wall', 0.7771165370941162)\n",
      "\n",
      "Water Similar:\n",
      "('wood', 0.6504098773002625)\n",
      "('waters', 0.6160298585891724)\n",
      "('ashes', 0.6140490174293518)\n",
      "('root', 0.6133626103401184)\n",
      "('smoke', 0.6048316955566406)\n",
      "('streams', 0.5931802988052368)\n",
      "('rivers', 0.5920335054397583)\n",
      "('brook', 0.5886960029602051)\n",
      "('trees', 0.5876567363739014)\n",
      "('pit', 0.5868188738822937)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Results for window=10\n",
    "\n",
    "bible_sim = model1_v3.wv.most_similar(\"bible\")[:10]\n",
    "book_sim = model1_v3.wv.most_similar(\"book\")[:10]\n",
    "bank_sim = model1_v3.wv.most_similar(\"bank\")[:10]\n",
    "water_sim = model1_v3.wv.most_similar(\"water\")[:10]\n",
    "\n",
    "print(\"Bible Similar:\")\n",
    "for i in bible_sim:\n",
    "    print(i)\n",
    "print()\n",
    "print(\"Book Similar:\")\n",
    "for i in book_sim:\n",
    "    print(i)\n",
    "print()\n",
    "\n",
    "print(\"Bank Similar:\")\n",
    "for i in bank_sim:\n",
    "    print(i)\n",
    "print()\n",
    "\n",
    "print(\"Water Similar:\")\n",
    "for i in water_sim:\n",
    "    print(i)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "eac5f05c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bible Similar:\n",
      "('slavery', 0.701246440410614)\n",
      "('patriot', 0.693037748336792)\n",
      "('royalty', 0.6879929900169373)\n",
      "('eidolons', 0.6872194409370422)\n",
      "('amariah', 0.6867314577102661)\n",
      "('baron', 0.6855996251106262)\n",
      "('achilles', 0.684070348739624)\n",
      "('tragedie', 0.6839808821678162)\n",
      "('xi', 0.681718647480011)\n",
      "('aristarchus', 0.6763778328895569)\n",
      "\n",
      "Book Similar:\n",
      "('chapter', 0.655816376209259)\n",
      "('chronicles', 0.6072105169296265)\n",
      "('note', 0.6068582534790039)\n",
      "('letter', 0.6044731736183167)\n",
      "('written', 0.6022891998291016)\n",
      "('volume', 0.560756504535675)\n",
      "('law', 0.5313780903816223)\n",
      "('vision', 0.5311765670776367)\n",
      "('writing', 0.5204434394836426)\n",
      "('temple', 0.5164452791213989)\n",
      "\n",
      "Bank Similar:\n",
      "('floor', 0.8228034973144531)\n",
      "('beach', 0.802124559879303)\n",
      "('lawn', 0.7908689975738525)\n",
      "('top', 0.7808291912078857)\n",
      "('hedge', 0.7632803916931152)\n",
      "('bay', 0.7524194717407227)\n",
      "('pavement', 0.7513832449913025)\n",
      "('road', 0.7498458623886108)\n",
      "('crossing', 0.7413783073425293)\n",
      "('hill', 0.7399481534957886)\n",
      "\n",
      "Water Similar:\n",
      "('wood', 0.6651054620742798)\n",
      "('waters', 0.6112533807754517)\n",
      "('ashes', 0.6062209606170654)\n",
      "('streams', 0.6033791899681091)\n",
      "('pit', 0.5971135497093201)\n",
      "('smoke', 0.5882624983787537)\n",
      "('trees', 0.5843954086303711)\n",
      "('furnace', 0.580277144908905)\n",
      "('brooks', 0.5800072550773621)\n",
      "('field', 0.578273355960846)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Results for window=15\n",
    "\n",
    "bible_sim = model1_v4.wv.most_similar(\"bible\")[:10]\n",
    "book_sim = model1_v4.wv.most_similar(\"book\")[:10]\n",
    "bank_sim = model1_v4.wv.most_similar(\"bank\")[:10]\n",
    "water_sim = model1_v4.wv.most_similar(\"water\")[:10]\n",
    "\n",
    "print(\"Bible Similar:\")\n",
    "for i in bible_sim:\n",
    "    print(i)\n",
    "print()\n",
    "print(\"Book Similar:\")\n",
    "for i in book_sim:\n",
    "    print(i)\n",
    "print()\n",
    "\n",
    "print(\"Bank Similar:\")\n",
    "for i in bank_sim:\n",
    "    print(i)\n",
    "print()\n",
    "\n",
    "print(\"Water Similar:\")\n",
    "for i in water_sim:\n",
    "    print(i)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8838ebcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for (girls,queens,kings): kings ,distance: 0.6486280560493469\n",
      "Result for (good,taller,tall): good ,distance: 0.5230245590209961\n",
      "Result for (france,paris,london): france ,distance: 0.6676658987998962\n"
     ]
    }
   ],
   "source": [
    "# d) Semantic analogy of given triplets of words\n",
    "\n",
    "v1 = model1.wv[\"girls\"] - model1.wv[\"queen\"] + model1.wv[\"kings\"]\n",
    "v1_sim = model1.wv.most_similar(positive=[v1])[:1]\n",
    "print(\"Result for (girls,queens,kings):\",v1_sim[0][0],\",distance:\",v1_sim[0][1])\n",
    "\n",
    "v2= model1.wv[\"good\"]- model1.wv[\"taller\"] + model1.wv[\"tall\"]\n",
    "v2_sim = model1.wv.most_similar(positive=[v2])[:1]\n",
    "print(\"Result for (good,taller,tall):\",v2_sim[0][0],\",distance:\",v2_sim[0][1])\n",
    "\n",
    "v3= model1.wv[\"france\"]- model1.wv[\"paris\"] + model1.wv[\"london\"]\n",
    "v3_sim = model1.wv.most_similar(positive=[v3])[:1]\n",
    "print(\"Result for (france,paris,london):\",v3_sim[0][0],\",distance:\",v3_sim[0][1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6cca0971",
   "metadata": {},
   "outputs": [],
   "source": [
    "# e) Load pretrained GoogleNews vectors\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "model_google = KeyedVectors.load_word2vec_format('./GoogleNews-vectors-negative300.bin', binary=True, limit=1000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "6743da80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bible Similar:\n",
      "('Bible', 0.736778199672699)\n",
      "('bibles', 0.6052598357200623)\n",
      "('Holy_Bible', 0.5989601612091064)\n",
      "('scriptures', 0.574568510055542)\n",
      "('scripture', 0.5697901844978333)\n",
      "('New_Testament', 0.5638793110847473)\n",
      "('Scripture', 0.5502957701683044)\n",
      "('Scriptures', 0.5411645770072937)\n",
      "('NRSV', 0.5341106057167053)\n",
      "('Leviticus_##:##-##', 0.5247005224227905)\n",
      "\n",
      "Book Similar:\n",
      "('tome', 0.7485830783843994)\n",
      "('books', 0.7379177808761597)\n",
      "('memoir', 0.7302926778793335)\n",
      "('paperback_edition', 0.6868364214897156)\n",
      "('autobiography', 0.6741527318954468)\n",
      "('memoirs', 0.6505153179168701)\n",
      "('Book', 0.6479282975196838)\n",
      "('paperback', 0.6471226811408997)\n",
      "('novels', 0.6341459155082703)\n",
      "('hardback', 0.6283079981803894)\n",
      "\n",
      "Bank Similar:\n",
      "('banks', 0.7440759539604187)\n",
      "('banking', 0.690161406993866)\n",
      "('Bank', 0.6698698401451111)\n",
      "('lender', 0.6342284679412842)\n",
      "('banker', 0.6092953085899353)\n",
      "('depositors', 0.6031531691551208)\n",
      "('mortgage_lender', 0.5797975659370422)\n",
      "('depositor', 0.5716427564620972)\n",
      "('BofA', 0.5714625120162964)\n",
      "('Citibank', 0.5589520335197449)\n",
      "\n",
      "Water Similar:\n",
      "('potable_water', 0.6799106001853943)\n",
      "('Water', 0.6706871390342712)\n",
      "('sewage', 0.6619377732276917)\n",
      "('groundwater', 0.6588346362113953)\n",
      "('Floridan_aquifer', 0.6422534584999084)\n",
      "('freshwater', 0.6307883262634277)\n",
      "('potable', 0.6251927614212036)\n",
      "('wastewater', 0.6212229132652283)\n",
      "('brackish_groundwater', 0.6206730604171753)\n",
      "('aquifer', 0.6143589615821838)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# st) Top 5 similar words to bank,book,bible,water\n",
    "\n",
    "bible_sim2 = model_google.most_similar(\"bible\")[:10]\n",
    "book_sim2 = model_google.most_similar(\"book\")[:10]\n",
    "bank_sim2 = model_google.most_similar(\"bank\")[:10]\n",
    "water_sim2 = model_google.most_similar(\"water\")[:10]\n",
    "\n",
    "print(\"Bible Similar:\")\n",
    "for i in bible_sim2:\n",
    "    print(i)\n",
    "print()\n",
    "print(\"Book Similar:\")\n",
    "for i in book_sim2:\n",
    "    print(i)\n",
    "print()\n",
    "\n",
    "print(\"Bank Similar:\")\n",
    "for i in bank_sim2:\n",
    "    print(i)\n",
    "print()\n",
    "\n",
    "print(\"Water Similar:\")\n",
    "for i in water_sim2:\n",
    "    print(i)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1af0d61e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for (girls,queens,kings): boys ,distance: 0.6931698322296143\n",
      "Result for (good,taller,tall): good ,distance: 0.6434131860733032\n",
      "Result for (france,paris,london): london ,distance: 0.754153847694397\n"
     ]
    }
   ],
   "source": [
    "# z) Semantic analogy of given triplets of words\n",
    "\n",
    "v1_google = model_google[\"girls\"] - model_google[\"queen\"] + model_google[\"kings\"]\n",
    "v1_sim_google = model_google.most_similar(positive=[v1_google])[:1]\n",
    "print(\"Result for (girls,queens,kings):\",v1_sim_google[0][0],\",distance:\",v1_sim_google[0][1])\n",
    "\n",
    "v2_google= model_google[\"good\"]- model_google[\"taller\"] + model_google[\"tall\"]\n",
    "v2_sim_google = model_google.most_similar(positive=[v2_google])[:1]\n",
    "print(\"Result for (good,taller,tall):\",v2_sim_google[0][0],\",distance:\",v2_sim_google[0][1])\n",
    "\n",
    "v3_google= model_google[\"france\"]- model_google[\"paris\"] + model_google[\"london\"]\n",
    "v3_sim_google = model_google.most_similar(positive=[v3_google])[:1]\n",
    "print(\"Result for (france,paris,london):\",v3_sim_google[0][0],\",distance:\",v3_sim_google[0][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5af638a8",
   "metadata": {},
   "source": [
    "### Βήμα 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ef30be7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# b) Save word embeddings\n",
    "# first, find all words in vocabulary\n",
    "location = \"vocab/words.vocab.txt\"\n",
    "file = open(location,\"r\")\n",
    "lines = file.readlines()\n",
    "all_words = []\n",
    "for line in lines:\n",
    "    all_words.append(line.split()[0])\n",
    "file.close()\n",
    "\n",
    "# now lets find these word's embeddings\n",
    "file1 = open(\"data/embeddings.tsv\",\"w\")\n",
    "file2 = open(\"data/metadata.tsv\",\"w\")\n",
    "\n",
    "for word in all_words:\n",
    "    for value in model1.wv[word][:-2]:\n",
    "        file1.write(str(value)+\"\\t\")\n",
    "    file1.write(str(model1.wv[word][-1]))\n",
    "    file1.write(\"\\n\")\n",
    "    file2.write(str(word)+\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e877bf64",
   "metadata": {},
   "source": [
    "### Βήμα 14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "707bb6c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions from w2v_sentiment_analysis.py\n",
    "import glob\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import sklearn\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "\n",
    "data_dir = os.path.join(\".\", \"data/aclImdb/\")\n",
    "train_dir = os.path.join(data_dir, \"train\")\n",
    "test_dir = os.path.join(data_dir, \"test\")\n",
    "pos_train_dir = os.path.join(train_dir, \"pos\")\n",
    "neg_train_dir = os.path.join(train_dir, \"neg\")\n",
    "pos_test_dir = os.path.join(test_dir, \"pos\")\n",
    "neg_test_dir = os.path.join(test_dir, \"neg\")\n",
    "\n",
    "# For memory limitations. These parameters fit in 8GB of RAM.\n",
    "# If you have 16G of RAM you can experiment with the full dataset / W2V\n",
    "MAX_NUM_SAMPLES = 5000\n",
    "# Load first 1M word embeddings. This works because GoogleNews are roughly\n",
    "# sorted from most frequent to least frequent.\n",
    "# It may yield much worse results for other embeddings corpora\n",
    "NUM_W2V_TO_LOAD = 1000000\n",
    "\n",
    "\n",
    "SEED = 42\n",
    "\n",
    "# Fix numpy random seed for reproducibility\n",
    "np.random.seed(SEED)\n",
    "\n",
    "\n",
    "def strip_punctuation(s):\n",
    "    return re.sub(r\"[^a-zA-Z\\s]\", \" \", s)\n",
    "\n",
    "\n",
    "def preprocess(s):\n",
    "    return re.sub(\"\\s+\", \" \", strip_punctuation(s).lower())\n",
    "\n",
    "\n",
    "def tokenize(s):\n",
    "    return s.split(\" \")\n",
    "\n",
    "\n",
    "def preproc_tok(s):\n",
    "    return tokenize(preprocess(s))\n",
    "\n",
    "\n",
    "def read_samples(folder, preprocess=lambda x: x):\n",
    "    samples = glob.iglob(os.path.join(folder, \"*.txt\"))\n",
    "    data = []\n",
    "\n",
    "    for i, sample in enumerate(samples):\n",
    "        if MAX_NUM_SAMPLES > 0 and i == MAX_NUM_SAMPLES:\n",
    "            break\n",
    "        with open(sample, \"r\") as fd:\n",
    "            x = [preprocess(l) for l in fd][0]\n",
    "            data.append(x)\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def create_corpus(pos, neg):\n",
    "    corpus = np.array(pos + neg, dtype=object)\n",
    "    y = np.array([1 for _ in pos] + [0 for _ in neg])\n",
    "    indices = np.arange(y.shape[0])\n",
    "    np.random.shuffle(indices)\n",
    "\n",
    "    return list(corpus[indices]), list(y[indices])\n",
    "\n",
    "\n",
    "def extract_nbow(corpus, model_name, vector_size = 100):\n",
    "    nbow = []\n",
    "    # for each sentence in corpus\n",
    "    for sent in corpus:\n",
    "        cnt =0\n",
    "        vec = np.zeros(vector_size)  #in this vector we calculate the sum \n",
    "        index2word_set = set(model_name.index_to_key)\n",
    "        \n",
    "        #for each word in sentence add its vector to sum if word is in vocabulary\n",
    "        #otherwise don't add which is equivalent with adding a zero vector\n",
    "        for word in sent:\n",
    "            cnt +=1\n",
    "            \n",
    "            if word in index2word_set:\n",
    "                vec = np.add(vec, model_name[word])\n",
    "        \n",
    "        #cnt is the total number of words, so we divide our vector with cnt to calculate the average \n",
    "        vec = np.divide(vec, cnt) \n",
    "        nbow.append(vec)\n",
    "            \n",
    "    return nbow\n",
    "\n",
    "def train_sentiment_analysis(train_corpus, train_labels):\n",
    "    \"\"\"Train a sentiment analysis classifier using NBOW + Logistic regression\"\"\"\n",
    "    regression_model = LogisticRegression()\n",
    "    regression_model.fit(train_corpus, train_labels)\n",
    "    \n",
    "    return regression_model\n",
    "    raise NotImplementedError(\"Implement sentiment analysis training\")\n",
    "\n",
    "\n",
    "def evaluate_sentiment_analysis(classifier, test_corpus, test_labels):\n",
    "    \"\"\"Evaluate classifier in the test corpus and report accuracy\"\"\"\n",
    "    return sklearn.metrics.accuracy_score(test_labels, classifier.predict(test_corpus))\n",
    "    raise NotImplementedError(\"Implement sentiment analysis evaluation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6e030d89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.7353\n"
     ]
    }
   ],
   "source": [
    "#use the model that we trained with gutenberg.txt\n",
    "model1=Word2Vec.load(\"word2vec.model1\")\n",
    "\n",
    "#Read train corpus from acImdb folder and create nbow\n",
    "pos_train_corpus = read_samples(pos_train_dir, preproc_tok)\n",
    "neg_train_corpus = read_samples(neg_train_dir, preproc_tok)\n",
    "train_corpus, train_labels = create_corpus(pos_train_corpus, neg_train_corpus)\n",
    "nbow_corpus = extract_nbow(train_corpus, model1.wv)\n",
    "\n",
    "#Create a logistic regression model for classifying\n",
    "regression_model = train_sentiment_analysis(nbow_corpus, train_labels)\n",
    "\n",
    "#Read test corpus from acImdb folder and create nbow\n",
    "pos_test_corpus = read_samples(pos_test_dir, preproc_tok)\n",
    "neg_test_corpus = read_samples(neg_test_dir, preproc_tok)\n",
    "test_corpus, test_labels = create_corpus(pos_test_corpus, neg_test_corpus)\n",
    "nbow_test_corpus = extract_nbow(test_corpus, model1.wv)\n",
    "\n",
    "#Calculate the accuracy of our model\n",
    "print(\"Accuracy: \", evaluate_sentiment_analysis(regression_model, nbow_test_corpus, test_labels))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b023feab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.8323\n"
     ]
    }
   ],
   "source": [
    "#Create nbow for GoogleNews' model\n",
    "nbow_corpus_google = extract_nbow(train_corpus, model_google, 300)\n",
    "\n",
    "# Create a logistic regression model for classifying based on the nbow we created with GoogleNews' model\n",
    "regression_model_google = train_sentiment_analysis(nbow_corpus_google, train_labels)\n",
    "\n",
    "#Create nbow for test data\n",
    "nbow_test_corpus_google = extract_nbow(test_corpus, model_google, 300)\n",
    "\n",
    "#Calculate the accuracy of GoogleNews' model\n",
    "print(\"Accuracy: \", evaluate_sentiment_analysis(regression_model_google, nbow_test_corpus_google, test_labels))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
